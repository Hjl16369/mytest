import streamlit as st
import pandas as pd
import zipfile
import tempfile
import os
import shutil
from datetime import datetime
from io import BytesIO
import warnings

warnings.filterwarnings("ignore", message="missing ScriptRunContext")
warnings.filterwarnings("ignore", message="ScriptRunContext")
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

st.set_page_config(
    page_title="æ­£æŒè®¯å•†ä¸šæµå‘æ•°æ®AIå¤„ç†ç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å®¢æˆ·åç§°å¯¹åº”å…³ç³»å­—å…¸
customer_alias_mapping = {
    'å››å·åŒ»è¯æ€»éƒ¨': 'å›½è¯æ§è‚¡å››å·åŒ»è¯è‚¡ä»½æœ‰é™å…¬å¸',
    'å›½æ§æ”€æèŠ±æ€»éƒ¨': 'å›½è¯æ§è‚¡å››å·æ”€æèŠ±åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§ç”˜å­œæ€»éƒ¨': 'å›½è¯æ§è‚¡ç”˜å­œå·åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§å¹¿å®‰æ€»éƒ¨': 'å›½è¯æ§è‚¡å¹¿å®‰æœ‰é™å…¬å¸',
    'å›½æ§è¾¾å·æ€»éƒ¨': 'å›½è¯æ§è‚¡è¾¾å·æœ‰é™å…¬å¸',
    'å›½æ§ä¹å±±æ€»éƒ¨': 'å›½è¯æ§è‚¡(ä¹å±±)å·´èœ€è¯åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§å‡‰å±±æ€»éƒ¨': 'å›½è¯æ§è‚¡å‡‰å±±åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§çœ‰å±±æ€»éƒ¨': 'å›½è¯æ§è‚¡çœ‰å±±åŒ»è¯æœ‰é™å…¬å¸'
}

# äº§å“æ˜ å°„å­—å…¸
product_mapping = {
    'å¥¥å¡è¥¿å¹³ç‰‡(30S)': {
        'å•†å“åç§°': ['å¥¥å¡è¥¿å¹³ç‰‡', 'å¥¥å¡è¥¿å¹³ç‰‡(30S)'],
        'è§„æ ¼': ['0.3g*30ç‰‡', '0.3g/ç‰‡*10ç‰‡/æ¿*3æ¿/ç›’*200ç›’'],
        'å•ä½æ¢ç®—ç³»æ•°': {
            '0.3g*30ç‰‡': 1,
            '0.3g/ç‰‡*10ç‰‡/æ¿*3æ¿/ç›’*200ç›’': 1,
            'default': 1
        }
    },
    'å¸ƒæ´›èŠ¬ï¼ˆ100mlï¼‰': {
        'å•†å“åç§°': ['å¸ƒæ´›èŠ¬æ··æ‚¬æ¶²(è¿ªå°”è¯º)'],
        'è§„æ ¼': ['2%*100mlï¼š2.0g/ç“¶/ç›’'],
        'å•ä½æ¢ç®—ç³»æ•°': {
            '2%*100mlï¼š2.0g/ç“¶/ç›’': 1,
            'default': 1
        }
    },
    'å°¿æ¿€é…¶ï¼ˆ10ä¸‡å•ä½ï¼‰': {
        'å•†å“åç§°': ['æ³¨å°„ç”¨å°¿æ¿€é…¶'],
        'è§„æ ¼': ['10ä¸‡iuÃ—5ç“¶/ç›’'],
        'å•ä½æ¢ç®—ç³»æ•°': {
            '10ä¸‡iuÃ—5ç“¶/ç›’': 5,
            'default': 5
        }
    }
}

def get_conversion_factor(product_name, spec):
    try:
        if product_name in product_mapping:
            conversion_factors = product_mapping[product_name]['å•ä½æ¢ç®—ç³»æ•°']
            if spec and spec in conversion_factors:
                return conversion_factors[spec]
            return conversion_factors.get('default', 1)
        return 1
    except Exception as e:
        print(f"è·å–æ¢ç®—ç³»æ•°æ—¶å‡ºé”™: {e}")
        return 1

def create_reverse_mappings():
    reverse_customer_mapping = {v: k for k, v in customer_alias_mapping.items()}
    reverse_product_mapping = {}
    for out_product_name, product_info in product_mapping.items():
        for sales_product_name in product_info['å•†å“åç§°']:
            reverse_product_mapping[sales_product_name] = out_product_name
            for spec in product_info['è§„æ ¼']:
                combined_key = f"{sales_product_name}|{spec}"
                reverse_product_mapping[combined_key] = out_product_name
    return reverse_customer_mapping, reverse_product_mapping

def create_record_key(record):
    try:
        date_str = pd.to_datetime(record['å‡ºåº“æ—¥æœŸ']).strftime('%Y-%m-%d') if pd.notna(record['å‡ºåº“æ—¥æœŸ']) else ''
        key = (
            date_str,
            str(record['å•†ä¸šå…¬å¸']).strip(),
            str(record['äº§å“åç§°']).strip(), 
            str(record['æ‰¹å·']).strip(),
            float(record['æ•°é‡']) if pd.notna(record['æ•°é‡']) else 0.0
        )
        return key
    except Exception as e:
        print(f"åˆ›å»ºè®°å½•é”®å¤±è´¥: {e}")
        return None

def is_duplicate_record(new_record, existing_records_df):
    if existing_records_df.empty:
        return False
    new_key = create_record_key(new_record)
    if new_key is None:
        return False
    for _, existing_row in existing_records_df.iterrows():
        existing_key = create_record_key(existing_row)
        if existing_key == new_key:
            return True
    return False

def find_matching_sales_data(row, sales_detail_df, reverse_customer_mapping, reverse_product_mapping):
    try:
        out_company = str(row['å•†ä¸šå…¬å¸']).strip()
        out_product = str(row['äº§å“åç§°']).strip()
        out_batch = str(row['æ‰¹å·']).strip()
        
        # å…¬å¸åç§°åŒ¹é…
        company_matched_rows = []
        for idx, sales_row in sales_detail_df.iterrows():
            sales_company = str(sales_row['å…¬å¸åç§°']).strip()
            if sales_company in customer_alias_mapping:
                mapped_company = customer_alias_mapping[sales_company]
                if mapped_company == out_company:
                    company_matched_rows.append(idx)
        
        if not company_matched_rows:
            return pd.DataFrame()
        
        company_matched_df = sales_detail_df.loc[company_matched_rows].copy()
        
        # äº§å“åç§°åŒ¹é…
        product_matched_rows = []
        for idx, sales_row in company_matched_df.iterrows():
            sales_product = str(sales_row['å•†å“åç§°']).strip()
            sales_spec = str(sales_row['è§„æ ¼']).strip() if 'è§„æ ¼' in sales_row and pd.notna(sales_row['è§„æ ¼']) else ''
            
            match_found = False
            if sales_product in reverse_product_mapping:
                if reverse_product_mapping[sales_product] == out_product:
                    match_found = True
            
            if not match_found and sales_spec:
                combined_key = f"{sales_product}|{sales_spec}"
                if combined_key in reverse_product_mapping:
                    if reverse_product_mapping[combined_key] == out_product:
                        match_found = True
            
            if not match_found:
                for map_key, map_info in product_mapping.items():
                    if map_key == out_product:
                        if sales_product in map_info['å•†å“åç§°']:
                            if not sales_spec or sales_spec in map_info['è§„æ ¼']:
                                match_found = True
                                break
            
            if match_found:
                product_matched_rows.append(idx)
        
        if not product_matched_rows:
            return pd.DataFrame()
        
        product_matched_df = sales_detail_df.loc[product_matched_rows].copy()
        
        # æ‰¹å·ç²¾ç¡®åŒ¹é…
        batch_matched_df = product_matched_df[
            product_matched_df['æ‰¹å·'].astype(str).str.strip() == out_batch
        ].copy()
        
        return batch_matched_df
        
    except Exception as e:
        print(f"åŒ¹é…è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return pd.DataFrame()

def calculate_converted_quantity(sales_quantity, out_product, sales_spec):
    try:
        conversion_factor = get_conversion_factor(out_product, sales_spec)
        converted_quantity = sales_quantity * conversion_factor
        return converted_quantity
    except Exception as e:
        print(f"è®¡ç®—è½¬æ¢æ•°é‡æ—¶å‡ºé”™: {e}")
        return sales_quantity

def is_company_like(name):
    if not name or not isinstance(name, str):
        return False
    name = name.strip()
    company_suffixes = [
        'æœ‰é™å…¬å¸', 'æœ‰é™è´£ä»»å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸', 'é›†å›¢æœ‰é™å…¬å¸',
        'åŒ»è¯å…¬å¸', 'è¯ä¸šå…¬å¸', 'è¯æˆ¿', 'è¯Šæ‰€', 'åŒ»é™¢'
    ]
    return any(name.endswith(suffix) for suffix in company_suffixes)

def find_previous_level_company(product_name, batch_no, previous_level, direct_sale_df):
    try:
        matched_records = direct_sale_df[
            (direct_sale_df['äº§å“åç§°'].astype(str).str.strip() == str(product_name).strip()) &
            (direct_sale_df['æ‰¹å·'].astype(str).str.strip() == str(batch_no).strip()) &
            (direct_sale_df['çº§æ¬¡'] == previous_level)
        ]
        
        if not matched_records.empty:
            previous_company = str(matched_records.iloc[0]['å•†ä¸šå…¬å¸']).strip()
            return previous_company
        else:
            return ''
    except Exception as e:
        print(f"æŸ¥æ‰¾ä¸Šä¸€çº§å•†ä¸šå…¬å¸æ—¶å‡ºé”™: {e}")
        return ''

def process_flow_data_with_fixed_matching(direct_sale_df, sales_detail_df):
    reverse_customer_mapping, reverse_product_mapping = create_reverse_mappings()
    
    flow_template_cols = [
        'æµå‘å•†ä¸šå…¬å¸å', 'ä¾›è´§æ–¹', 'æ‰€å±æœˆä»½', 'å•æ®æ—¥æœŸ', 'ä»£ç†å•†', 
        'ä¸€çº§å•†ä¸šåç§°', 'äºŒçº§å•†ä¸šåç§°', 'ä¸‰çº§å•†ä¸šåç§°', 'å››çº§å•†ä¸šåç§°', 
        'ç»ˆç«¯åç§°', 'å“è§„', 'æ‰¹å·', 'é”€å”®æ•°é‡', 'è½¬æ¢åæ•°é‡', 'æ¢ç®—ç³»æ•°', 'åŸå§‹è§„æ ¼', 'æµå‘çº§åˆ«'
    ]
    
    flow_template_df = pd.DataFrame(columns=flow_template_cols)
    next_level_data = []
    existing_records_keys = set()
    
    for level in range(1, 5):
        current_level_df = direct_sale_df[direct_sale_df['çº§æ¬¡'] == level].copy()
        
        if current_level_df.empty:
            continue
        
        level_processed_count = 0
        
        for _, row in current_level_df.iterrows():
            try:
                matched_sales = find_matching_sales_data(
                    row, sales_detail_df, reverse_customer_mapping, reverse_product_mapping
                )
                
                if matched_sales.empty:
                    continue
                
                for _, sales_row in matched_sales.iterrows():
                    try:
                        out_date = pd.to_datetime(row['å‡ºåº“æ—¥æœŸ']) if pd.notna(row['å‡ºåº“æ—¥æœŸ']) else pd.Timestamp.now()
                        sales_quantity = pd.to_numeric(sales_row['é”€å”®æ•°é‡'], errors='coerce')
                        if pd.isna(sales_quantity):
                            sales_quantity = 0
                        
                        sales_spec = str(sales_row['è§„æ ¼']).strip() if 'è§„æ ¼' in sales_row and pd.notna(sales_row['è§„æ ¼']) else ''
                        out_product = str(row['äº§å“åç§°']).strip()
                        
                        converted_quantity = calculate_converted_quantity(sales_quantity, out_product, sales_spec)
                        conversion_factor = get_conversion_factor(out_product, sales_spec)
                        
                        new_row = {
                            'æµå‘å•†ä¸šå…¬å¸å': str(row['å•†ä¸šå…¬å¸']).strip(),
                            'ä¾›è´§æ–¹': "",
                            'æ‰€å±æœˆä»½': out_date.strftime('%Y-%m'),
                            'å•æ®æ—¥æœŸ': out_date,
                            'ä»£ç†å•†': "",
                            'ä¸€çº§å•†ä¸šåç§°': '',
                            'äºŒçº§å•†ä¸šåç§°': '',
                            'ä¸‰çº§å•†ä¸šåç§°': '',
                            'å››çº§å•†ä¸šåç§°': '',
                            'ç»ˆç«¯åç§°': str(sales_row['å®¢æˆ·åç§°']).strip(),
                            'å“è§„': str(row['äº§å“åç§°']).strip(),
                            'æ‰¹å·': str(row['æ‰¹å·']).strip(),
                            'é”€å”®æ•°é‡': sales_quantity,
                            'è½¬æ¢åæ•°é‡': converted_quantity,
                            'æ¢ç®—ç³»æ•°': conversion_factor,
                            'åŸå§‹è§„æ ¼': sales_spec,
                            'æµå‘çº§åˆ«': level
                        }
                        
                        level_key = f'{["", "ä¸€", "äºŒ", "ä¸‰", "å››"][level]}çº§å•†ä¸šåç§°'
                        new_row[level_key] = str(row['å•†ä¸šå…¬å¸']).strip()
                        
                        if level > 1:
                            previous_level = level - 1
                            previous_company = find_previous_level_company(
                                str(row['äº§å“åç§°']).strip(),
                                str(row['æ‰¹å·']).strip(), 
                                previous_level,
                                direct_sale_df
                            )
                            
                            if previous_company:
                                previous_level_key = f'{["", "ä¸€", "äºŒ", "ä¸‰", "å››"][previous_level]}çº§å•†ä¸šåç§°'
                                new_row[previous_level_key] = previous_company
                        
                        flow_template_df = pd.concat([flow_template_df, pd.DataFrame([new_row])], ignore_index=True)
                        level_processed_count += 1
                        
                        customer_name = str(sales_row['å®¢æˆ·åç§°']).strip()
                        if level < 4 and is_company_like(customer_name):
                            next_level_row = {
                                'å‡ºåº“æ—¥æœŸ': row['å‡ºåº“æ—¥æœŸ'],
                                'å•†ä¸šå…¬å¸': customer_name,
                                'äº§å“åç§°': row['äº§å“åç§°'],
                                'æ‰¹å·': row['æ‰¹å·'],
                                'æ•°é‡': converted_quantity,
                                'çº§æ¬¡': level + 1
                            }
                            
                            new_record_key = create_record_key(next_level_row)
                            
                            if new_record_key is not None:
                                if not is_duplicate_record(next_level_row, direct_sale_df):
                                    if new_record_key not in existing_records_keys:
                                        next_level_data.append(next_level_row)
                                        existing_records_keys.add(new_record_key)
                    
                    except Exception as e:
                        print(f"å¤„ç†é”€å”®è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
                        continue
            
            except Exception as e:
                print(f"å¤„ç†å‡ºåº“è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        if next_level_data:
            next_level_df = pd.DataFrame(next_level_data)
            direct_sale_df = pd.concat([direct_sale_df, next_level_df], ignore_index=True)
            
            for record in next_level_data:
                record_key = create_record_key(record)
                if record_key:
                    existing_records_keys.add(record_key)
                    
            next_level_data = []
    
    return flow_template_df

def read_excel_file(file_path):
    try:
        if file_path.endswith('.xlsx'):
            return pd.read_excel(file_path, engine='openpyxl')
        elif file_path.endswith('.xls'):
            try:
                return pd.read_excel(file_path, engine='openpyxl')
            except Exception:
                try:
                    return pd.read_excel(file_path, engine='xlrd')
                except Exception:
                    return pd.read_excel(file_path)
        else:
            return pd.read_excel(file_path)
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None

def safe_delete_file(file_path):
    try:
        if file_path and os.path.exists(file_path):
            os.unlink(file_path)
            return True
    except Exception as e:
        print(f"æ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}: {e}")
        return False

def safe_delete_directory(dir_path):
    try:
        if dir_path and os.path.exists(dir_path):
            shutil.rmtree(dir_path, ignore_errors=True)
            return True
    except Exception as e:
        print(f"æ— æ³•åˆ é™¤ç›®å½• {dir_path}: {e}")
        return False

def find_column_mapping(df_columns):
    mapping = {}
    column_mappings = {
        'date': ['å‡ºåº“æ—¥æœŸ', 'æ“ä½œæ—¥æœŸ', 'æ—¥æœŸ', 'å•æ®æ—¥æœŸ'],
        'company': ['å•†ä¸šå…¬å¸', 'è´­è´§å•ä½', 'å®¢æˆ·', 'å…¬å¸', 'å…¬å¸åç§°'],
        'product': ['äº§å“åç§°', 'äº§å“', 'å•†å“åç§°', 'å“å'],
        'batch': ['æ‰¹å·'],
        'quantity': ['æ•°é‡', 'æ‰¹å·å‡ºåº“æ•°é‡', 'å‡ºåº“æ•°é‡', 'é”€å”®æ•°é‡']
    }
    
    df_columns_str = [str(col).strip() for col in df_columns]
    
    for col_str in df_columns_str:
        for key, possible_names in column_mappings.items():
            if col_str in possible_names and key not in mapping:
                mapping[key] = col_str
                break
    
    return mapping

def validate_required_columns(sales_df, required_cols=['å…¬å¸åç§°', 'å•†å“åç§°', 'æ‰¹å·', 'é”€å”®æ•°é‡', 'å®¢æˆ·åç§°']):
    missing_cols = []
    for col in required_cols:
        if col not in sales_df.columns:
            missing_cols.append(col)
    return missing_cols

def process_files(zip_file, sales_file):
    temp_files = {}
    
    try:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("è¯»å–é”€å”®æ˜ç»†è¡¨...")
        progress_bar.progress(10)
        
        try:
            sales_temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')
            temp_files['sales_file'] = sales_temp_file.name
            sales_temp_file.write(sales_file.getvalue())
            sales_temp_file.close()
            
            sales_detail_df = read_excel_file(temp_files['sales_file'])
            if sales_detail_df is None:
                st.error("æ— æ³•è¯»å–é”€å”®æ˜ç»†è¡¨")
                return None, None
                
            sales_detail_df.columns = sales_detail_df.columns.str.strip()
            
            missing_cols = validate_required_columns(sales_detail_df)
            if missing_cols:
                st.error(f"é”€å”®æ˜ç»†è¡¨ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
                return None, None
            
            st.success(f"é”€å”®æ˜ç»†è¡¨è¯»å–æˆåŠŸï¼Œå…± {len(sales_detail_df)} è¡Œæ•°æ®")
            
        except Exception as e:
            st.error(f"è¯»å–é”€å”®æ˜ç»†è¡¨å¤±è´¥: {e}")
            return None, None
        
        status_text.text("è§£å‹å¹¶å¤„ç†å‡ºåº“æ˜ç»†å‹ç¼©åŒ…...")
        progress_bar.progress(20)
        
        zip_temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
        temp_files['zip_file'] = zip_temp_file.name
        zip_temp_file.write(zip_file.getvalue())
        zip_temp_file.close()
        
        try:
            extract_dir = tempfile.mkdtemp()
            temp_files['extract_dir'] = extract_dir
            
            with zipfile.ZipFile(temp_files['zip_file'], 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
                
            excel_files = [f for f in os.listdir(extract_dir) if f.endswith(('.xlsx', '.xls'))]
            st.info(f"å‹ç¼©åŒ…ä¸­åŒ…å« {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
            
            if not excel_files:
                st.warning("å‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°Excelæ–‡ä»¶")
                return None, None
            
            direct_sale_cols = ['å‡ºåº“æ—¥æœŸ', 'å•†ä¸šå…¬å¸', 'äº§å“åç§°', 'æ‰¹å·', 'æ•°é‡', 'çº§æ¬¡']
            direct_sale_df = pd.DataFrame(columns=direct_sale_cols)
            
            total_files = len(excel_files)
            processed_files = 0
            
            for file_name in excel_files:
                file_path = os.path.join(extract_dir, file_name)
                
                try:
                    df = read_excel_file(file_path)
                    if df is None or df.empty:
                        continue
                    
                    column_mapping = find_column_mapping(df.columns)
                    required_cols = ['date', 'company', 'product', 'batch', 'quantity']
                    missing_cols = [col for col in required_cols if col not in column_mapping]
                    
                    if missing_cols:
                        continue
                    
                    selected_df = df[[
                        column_mapping['date'],
                        column_mapping['company'],
                        column_mapping['product'],
                        column_mapping['batch'],
                        column_mapping['quantity']
                    ]].copy()
                    
                    selected_df.columns = ['å‡ºåº“æ—¥æœŸ', 'å•†ä¸šå…¬å¸', 'äº§å“åç§°', 'æ‰¹å·', 'æ•°é‡']
                    selected_df = selected_df.dropna(subset=['å•†ä¸šå…¬å¸', 'äº§å“åç§°', 'æ‰¹å·'])
                    selected_df['æ•°é‡'] = pd.to_numeric(selected_df['æ•°é‡'], errors='coerce').fillna(0)
                    selected_df['çº§æ¬¡'] = 1
                    
                    direct_sale_df = pd.concat([direct_sale_df, selected_df], ignore_index=True)
                    
                    processed_files += 1
                    progress_bar.progress(20 + int(60 * processed_files / total_files))
                    
                except Exception as e:
                    continue
            
            st.success(f"æˆåŠŸå¤„ç† {processed_files}/{total_files} ä¸ªæ–‡ä»¶ï¼Œè·å¾— {len(direct_sale_df)} æ¡å‡ºåº“è®°å½•")
            
        except Exception as e:
            st.error(f"è§£å‹æˆ–å¤„ç†å‹ç¼©åŒ…å¤±è´¥: {e}")
            return None, None
        
        if direct_sale_df.empty:
            st.warning("æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆçš„å‡ºåº“æ•°æ®")
            return None, None
        
        status_text.text("å¤„ç†æµå‘æ•°æ®...")
        progress_bar.progress(80)
        
        try:
            flow_template_df = process_flow_data_with_fixed_matching(direct_sale_df, sales_detail_df)
            
            if flow_template_df.empty:
                st.warning("æœªç”Ÿæˆä»»ä½•æµå‘æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®åŒ¹é…æƒ…å†µ")
                return None, None
            
            result_temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')
            temp_files['result_file'] = result_temp_file.name
            result_temp_file.close()
            
            flow_template_df.to_excel(temp_files['result_file'], index=False)
            
            progress_bar.progress(100)
            status_text.text("å¤„ç†å®Œæˆï¼")
            
            return temp_files['result_file'], temp_files
            
        except Exception as e:
            st.error(f"å¤„ç†æµå‘æ•°æ®å¤±è´¥: {e}")
            return None, None
            
    except Exception as e:
        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None, None

def cleanup_temp_files(temp_files):
    if not temp_files:
        return
    
    for key, file_path in temp_files.items():
        if key == 'extract_dir':
            safe_delete_directory(file_path)
        elif key != 'result_file':
            safe_delete_file(file_path)

def main():
    st.title("ğŸ“Š æµå‘æ•°æ®å¤„ç†AIç³»ç»Ÿ")
    st.markdown("---")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¦  å‚å®¶å‡ºåº“æ˜ç»†å‹ç¼©åŒ…")
        st.info("ä¸Šä¼ åŒ…å«å‡ºåº“æ˜ç»†Excelæ–‡ä»¶çš„ZIPå‹ç¼©åŒ…")
        zip_file = st.file_uploader("ä¸Šä¼ å‡ºåº“æ˜ç»†å‹ç¼©åŒ… (.zip)", type=['zip'], key='zip_uploader')
    
    with col2:
        st.subheader("ğŸ“‹ å•†ä¸šå…¬å¸é”€å”®æ˜ç»†è¡¨")
        st.info("ä¸Šä¼ å•†ä¸šå…¬å¸é”€å”®æ˜ç»†Excelæ–‡ä»¶")
        sales_file = st.file_uploader("ä¸Šä¼ å•†ä¸šå…¬å¸é”€å”®æ˜ç»†è¡¨ (.xlsx)", type=['xlsx'], key='sales_uploader')
    
    # å¤„ç†æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
        if zip_file is None or sales_file is None:
            st.warning("è¯·å…ˆä¸Šä¼ æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶ï¼")
            return
        
        temp_files_to_cleanup = None
        result_file_path = None
        
        try:
            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™..."):
                result_file_path, temp_files_to_cleanup = process_files(zip_file, sales_file)
                
                if result_file_path:
                    try:
                        result_df = pd.read_excel(result_file_path)
                        
                        st.success("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        col1, col2, col3, col4, col5 = st.columns(5)
                        
                        with col1:
                            st.metric("æ€»è®°å½•æ•°", len(result_df))
                        
                        with col2:
                            unique_companies = result_df['æµå‘å•†ä¸šå…¬å¸å'].nunique()
                            st.metric("æ¶‰åŠå•†ä¸šå…¬å¸æ•°", unique_companies)
                        
                        with col3:
                            unique_products = result_df['å“è§„'].nunique()
                            st.metric("æ¶‰åŠäº§å“æ•°", unique_products)
                            
                        with col4:
                            total_original_quantity = result_df['é”€å”®æ•°é‡'].sum()
                            st.metric("åŸå§‹é”€å”®æ•°é‡", f"{total_original_quantity:,.0f}")
                            
                        with col5:
                            total_converted_quantity = result_df['è½¬æ¢åæ•°é‡'].sum()
                            st.metric("è½¬æ¢åé”€å”®æ•°é‡", f"{total_converted_quantity:,.0f}")
                        
                        # æŒ‰æµå‘çº§åˆ«ç»Ÿè®¡
                        st.subheader("ğŸ“ˆ æµå‘çº§åˆ«ç»Ÿè®¡")
                        level_stats = result_df.groupby('æµå‘çº§åˆ«').agg({
                            'æµå‘å•†ä¸šå…¬å¸å': 'count',
                            'é”€å”®æ•°é‡': 'sum',
                            'è½¬æ¢åæ•°é‡': 'sum'
                        }).rename(columns={
                            'æµå‘å•†ä¸šå…¬å¸å': 'è®°å½•æ•°',
                            'é”€å”®æ•°é‡': 'åŸå§‹æ€»æ•°é‡',
                            'è½¬æ¢åæ•°é‡': 'è½¬æ¢åæ€»æ•°é‡'
                        })
                        st.dataframe(level_stats)
                        
                        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                        st.subheader("ğŸ“Š å¤„ç†ç»“æœé¢„è§ˆ")
                        st.dataframe(result_df.head(10))
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        st.subheader("ğŸ“¥ ä¸‹è½½å¤„ç†ç»“æœ")
                        
                        output = BytesIO()
                        with pd.ExcelWriter(output, engine='openpyxl') as writer:
                            result_df.to_excel(writer, index=False, sheet_name='æµå‘æ•°æ®')
                            level_stats.to_excel(writer, sheet_name='çº§åˆ«ç»Ÿè®¡')
                        
                        output.seek(0)
                        
                        download_filename = f"æµå‘æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                        
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½æ¸…æ´—å®Œæˆæµå‘æ•°æ®Excelæ–‡ä»¶",
                            data=output.getvalue(),
                            file_name=download_filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                        
                        st.success(f"æ–‡ä»¶å·²å‡†å¤‡å¥½ä¸‹è½½ï¼š{download_filename}")
                            
                    except Exception as e:
                        st.error(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
                else:
                    st.error("âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œæ•°æ®å†…å®¹")
                    
        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
            
        finally:
            if temp_files_to_cleanup:
                cleanup_temp_files(temp_files_to_cleanup)
            
            if result_file_path:
                safe_delete_file(result_file_path)

if __name__ == "__main__":
    main()