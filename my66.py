import streamlit as st
import pandas as pd
import zipfile
import tempfile
import os
import shutil
from datetime import datetime
from io import BytesIO
import warnings

# æ˜ç¡®çš„è­¦å‘Šè¿‡æ»¤è®¾ç½®
warnings.filterwarnings("ignore", message="missing ScriptRunContext")
warnings.filterwarnings("ignore", message="ScriptRunContext")
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æµå‘æ•°æ®AIå¤„ç†ç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å®¢æˆ·åç§°å¯¹åº”å…³ç³»å­—å…¸
customer_alias_mapping = {
    'å››å·åŒ»è¯æ€»éƒ¨': 'å›½è¯æ§è‚¡å››å·åŒ»è¯è‚¡ä»½æœ‰é™å…¬å¸',
    'å›½æ§æ”€æèŠ±æ€»éƒ¨': 'å›½è¯æ§è‚¡å››å·æ”€æèŠ±åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§ç”˜å­œæ€»éƒ¨': 'å›½è¯æ§è‚¡ç”˜å­œå·åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§å¹¿å®‰æ€»éƒ¨': 'å›½è¯æ§è‚¡å¹¿å®‰æœ‰é™å…¬å¸',
    'å›½æ§è¾¾å·æ€»éƒ¨': 'å›½è¯æ§è‚¡è¾¾å·æœ‰é™å…¬å¸',
    'å›½æ§ä¹å±±æ€»éƒ¨': 'å›½è¯æ§è‚¡(ä¹å±±)å·è¯åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§å‡‰å±±æ€»éƒ¨': 'å›½è¯æ§è‚¡å‡‰å±±åŒ»è¯æœ‰é™å…¬å¸',
    'å›½æ§çœ‰å±±æ€»éƒ¨': 'å›½è¯æ§è‚¡çœ‰å±±åŒ»è¯æœ‰é™å…¬å¸'
}

# äº§å“æ˜ å°„å­—å…¸ - æ–°å¢å•ä½æ¢ç®—ç³»æ•°
product_mapping = {
    'å¥¥å¡è¥¿å¹³ç‰‡(30S)': {
        'å•†å“åç§°': ['å¥¥å¡è¥¿å¹³ç‰‡', 'å¥¥å¡è¥¿å¹³ç‰‡(30S)'],
        'è§„æ ¼': ['0.3g*30ç‰‡', '0.3g/ç‰‡*10ç‰‡/æ¿*3æ¿/ç›’*200ç›’'],
        'å•ä½æ¢ç®—ç³»æ•°': {
            # åŸºå‡†å•ä½ï¼šç‰‡ï¼Œå‡ºåº“æ˜ç»†é€šå¸¸ä»¥ç›’ä¸ºå•ä½ï¼Œé”€å”®æ˜ç»†ä»¥ç‰‡ä¸ºå•ä½
            '0.3g*30ç‰‡': 1,  # 1ç›’ = 30ç‰‡
            '0.3g/ç‰‡*10ç‰‡/æ¿*3æ¿/ç›’*200ç›’': 1,  # 1ç®±(200ç›’) = 6000ç‰‡
            'default': 1  # é»˜è®¤æ¢ç®—ç³»æ•°
        }
    },
    'å¸ƒæ´›èŠ¬ï¼ˆ100mlï¼‰': {
        'å•†å“åç§°': ['å¸ƒæ´›èŠ¬æ··æ‚¬æ¶²(è¿ªå°”è¯º)'],
        'è§„æ ¼': ['2%*100mlï¼š2.0g/ç“¶/ç›’'],
        'å•ä½æ¢ç®—ç³»æ•°': {
            # åŸºå‡†å•ä½ï¼šç“¶
            '2%*100mlï¼š2.0g/ç“¶/ç›’': 1,  # 1ç›’ = 1ç“¶
            'default': 1  # é»˜è®¤æ¢ç®—ç³»æ•°
        }
    },
    'å°¿æ¿€é…¶ï¼ˆ10ä¸‡å•ä½ï¼‰': {
        'å•†å“åç§°': ['æ³¨å°„ç”¨å°¿æ¿€é…¶'],
        'è§„æ ¼': ['10ä¸‡iuÃ—5ç“¶/ç›’'],
        'å•ä½æ¢ç®—ç³»æ•°': {
            # åŸºå‡†å•ä½ï¼šç“¶
            '10ä¸‡iuÃ—5ç“¶/ç›’': 5,  # 1ç›’ = 5ç“¶
            'default': 5  # é»˜è®¤æ¢ç®—ç³»æ•°
        }
    }
}

def get_conversion_factor(product_name, spec):
    """
    è·å–äº§å“çš„å•ä½æ¢ç®—ç³»æ•°
    
    å‚æ•°:
    - product_name: å‡ºåº“æ˜ç»†ä¸­çš„äº§å“åç§°
    - spec: é”€å”®æ˜ç»†ä¸­çš„è§„æ ¼
    
    è¿”å›:
    - æ¢ç®—ç³»æ•° (float)
    """
    try:
        if product_name in product_mapping:
            conversion_factors = product_mapping[product_name]['å•ä½æ¢ç®—ç³»æ•°']
            
            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…è§„æ ¼
            if spec and spec in conversion_factors:
                return conversion_factors[spec]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤ç³»æ•°
            return conversion_factors.get('default', 1)
        
        # å¦‚æœäº§å“ä¸åœ¨æ˜ å°„ä¸­ï¼Œè¿”å›é»˜è®¤ç³»æ•°1
        return 1
        
    except Exception as e:
        print(f"è·å–æ¢ç®—ç³»æ•°æ—¶å‡ºé”™: {e}")
        return 1

def create_reverse_mappings():
    """åˆ›å»ºåå‘æ˜ å°„å­—å…¸ï¼Œç”¨äºä»é”€å”®æ˜ç»†åŒ¹é…åˆ°å‡ºåº“æ˜ç»†"""
    
    # åˆ›å»ºå®¢æˆ·åç§°åå‘æ˜ å°„ (å…¨ç§° -> ç®€ç§°)
    reverse_customer_mapping = {v: k for k, v in customer_alias_mapping.items()}
    
    # åˆ›å»ºäº§å“åç§°åå‘æ˜ å°„ - ä¿®æ­£é€»è¾‘
    reverse_product_mapping = {}
    for out_product_name, product_info in product_mapping.items():
        # å¤„ç†å•†å“åç§°åˆ—è¡¨
        for sales_product_name in product_info['å•†å“åç§°']:
            reverse_product_mapping[sales_product_name] = out_product_name
            
            # åŒæ—¶æ”¯æŒå•†å“åç§°+è§„æ ¼çš„ç»„åˆåŒ¹é…
            for spec in product_info['è§„æ ¼']:
                combined_key = f"{sales_product_name}|{spec}"
                reverse_product_mapping[combined_key] = out_product_name
    
    return reverse_customer_mapping, reverse_product_mapping

def create_record_key(record):
    """åˆ›å»ºè®°å½•çš„å”¯ä¸€æ ‡è¯†é”®ï¼Œç”¨äºå»é‡æ£€æŸ¥"""
    try:
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        date_str = pd.to_datetime(record['å‡ºåº“æ—¥æœŸ']).strftime('%Y-%m-%d') if pd.notna(record['å‡ºåº“æ—¥æœŸ']) else ''
        
        key = (
            date_str,
            str(record['å•†ä¸šå…¬å¸']).strip(),
            str(record['äº§å“åç§°']).strip(), 
            str(record['æ‰¹å·']).strip(),
            float(record['æ•°é‡']) if pd.notna(record['æ•°é‡']) else 0.0
        )
        return key
    except Exception as e:
        print(f"åˆ›å»ºè®°å½•é”®å¤±è´¥: {e}")
        return None

def is_duplicate_record(new_record, existing_records_df):
    """æ£€æŸ¥æ–°è®°å½•æ˜¯å¦ä¸ç°æœ‰è®°å½•é‡å¤"""
    if existing_records_df.empty:
        return False
    
    new_key = create_record_key(new_record)
    if new_key is None:
        return False
    
    # æ£€æŸ¥ç°æœ‰è®°å½•ä¸­æ˜¯å¦æœ‰ç›¸åŒçš„é”®
    for _, existing_row in existing_records_df.iterrows():
        existing_key = create_record_key(existing_row)
        if existing_key == new_key:
            return True
    
    return False

def find_matching_sales_data(row, sales_detail_df, reverse_customer_mapping, reverse_product_mapping):
    """
    ä¸ºå‡ºåº“æ˜ç»†çš„ä¸€è¡Œæ•°æ®æ‰¾åˆ°åŒ¹é…çš„é”€å”®æ˜ç»†æ•°æ®
    
    å‚æ•°:
    - row: å‡ºåº“æ˜ç»†çš„ä¸€è¡Œæ•°æ®
    - sales_detail_df: é”€å”®æ˜ç»†DataFrame
    - reverse_customer_mapping: å®¢æˆ·åç§°åå‘æ˜ å°„
    - reverse_product_mapping: äº§å“åç§°åå‘æ˜ å°„
    
    è¿”å›:
    - åŒ¹é…çš„é”€å”®æ˜ç»†DataFrame
    """
    try:
        # 1. ä»å‡ºåº“æ˜ç»†è·å–è¦åŒ¹é…çš„ä¿¡æ¯
        out_company = str(row['å•†ä¸šå…¬å¸']).strip()
        out_product = str(row['äº§å“åç§°']).strip()
        out_batch = str(row['æ‰¹å·']).strip()
        
        print(f"åŒ¹é…ç›®æ ‡ - å…¬å¸: {out_company}, äº§å“: {out_product}, æ‰¹å·: {out_batch}")
        
        # 2. å…¬å¸åç§°åŒ¹é… - ä¿®æ­£é€»è¾‘
        # åœ¨é”€å”®æ˜ç»†ä¸­æ‰¾åˆ°èƒ½æ˜ å°„åˆ°å‡ºåº“å…¬å¸çš„è®°å½•
        company_matched_rows = []
        for idx, sales_row in sales_detail_df.iterrows():
            sales_company = str(sales_row['å…¬å¸åç§°']).strip()
            # æ£€æŸ¥é”€å”®å…¬å¸æ˜¯å¦èƒ½æ˜ å°„åˆ°å‡ºåº“å…¬å¸
            if sales_company in customer_alias_mapping:
                mapped_company = customer_alias_mapping[sales_company]
                if mapped_company == out_company:
                    company_matched_rows.append(idx)
        
        if not company_matched_rows:
            print(f"æœªæ‰¾åˆ°åŒ¹é…çš„å…¬å¸: {out_company}")
            return pd.DataFrame()
        
        # è·å–å…¬å¸åŒ¹é…çš„æ•°æ®
        company_matched_df = sales_detail_df.loc[company_matched_rows].copy()
        
        # 3. äº§å“åç§°åŒ¹é… - ä¿®æ­£é€»è¾‘
        product_matched_rows = []
        for idx, sales_row in company_matched_df.iterrows():
            sales_product = str(sales_row['å•†å“åç§°']).strip()
            sales_spec = str(sales_row['è§„æ ¼']).strip() if 'è§„æ ¼' in sales_row and pd.notna(sales_row['è§„æ ¼']) else ''
            
            # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
            match_found = False
            
            # æ–¹å¼1: ç›´æ¥é€šè¿‡å•†å“åç§°åŒ¹é…
            if sales_product in reverse_product_mapping:
                if reverse_product_mapping[sales_product] == out_product:
                    match_found = True
            
            # æ–¹å¼2: é€šè¿‡å•†å“åç§°+è§„æ ¼ç»„åˆåŒ¹é…
            if not match_found and sales_spec:
                combined_key = f"{sales_product}|{sales_spec}"
                if combined_key in reverse_product_mapping:
                    if reverse_product_mapping[combined_key] == out_product:
                        match_found = True
            
            # æ–¹å¼3: åœ¨äº§å“æ˜ å°„å­—å…¸ä¸­ç›´æ¥æŸ¥æ‰¾
            if not match_found:
                for map_key, map_info in product_mapping.items():
                    if map_key == out_product:
                        # æ£€æŸ¥å•†å“åç§°æ˜¯å¦åŒ¹é…
                        if sales_product in map_info['å•†å“åç§°']:
                            # å¦‚æœæ²¡æœ‰è§„æ ¼ä¿¡æ¯ï¼Œæˆ–è§„æ ¼åŒ¹é…
                            if not sales_spec or sales_spec in map_info['è§„æ ¼']:
                                match_found = True
                                break
            
            if match_found:
                product_matched_rows.append(idx)
        
        if not product_matched_rows:
            print(f"æœªæ‰¾åˆ°åŒ¹é…çš„äº§å“: {out_product}")
            return pd.DataFrame()
        
        # è·å–äº§å“åŒ¹é…çš„æ•°æ®
        product_matched_df = sales_detail_df.loc[product_matched_rows].copy()
        
        # 4. æ‰¹å·ç²¾ç¡®åŒ¹é…
        batch_matched_df = product_matched_df[
            product_matched_df['æ‰¹å·'].astype(str).str.strip() == out_batch
        ].copy()
        
        if batch_matched_df.empty:
            print(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ‰¹å·: {out_batch}")
        else:
            print(f"æ‰¾åˆ° {len(batch_matched_df)} æ¡åŒ¹é…è®°å½•")
        
        return batch_matched_df
        
    except Exception as e:
        print(f"åŒ¹é…è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return pd.DataFrame()

def calculate_converted_quantity(sales_quantity, out_product, sales_spec):
    """
    æ ¹æ®å•ä½æ¢ç®—ç³»æ•°è®¡ç®—è½¬æ¢åçš„æ•°é‡
    
    å‚æ•°:
    - sales_quantity: åŸå§‹é”€å”®æ•°é‡
    - out_product: å‡ºåº“äº§å“åç§°
    - sales_spec: é”€å”®æ˜ç»†ä¸­çš„è§„æ ¼
    
    è¿”å›:
    - è½¬æ¢åçš„æ•°é‡
    """
    try:
        # è·å–æ¢ç®—ç³»æ•°
        conversion_factor = get_conversion_factor(out_product, sales_spec)
        
        # è®¡ç®—è½¬æ¢åçš„æ•°é‡
        converted_quantity = sales_quantity * conversion_factor
        
        print(f"æ•°é‡è½¬æ¢: åŸå§‹æ•°é‡={sales_quantity}, æ¢ç®—ç³»æ•°={conversion_factor}, è½¬æ¢åæ•°é‡={converted_quantity}")
        
        return converted_quantity
        
    except Exception as e:
        print(f"è®¡ç®—è½¬æ¢æ•°é‡æ—¶å‡ºé”™: {e}")
        return sales_quantity  # å‡ºé”™æ—¶è¿”å›åŸæ•°é‡

def is_company_like(name):
    """åˆ¤æ–­æ˜¯å¦ä¸ºå…¬å¸åç§°"""
    if not name or not isinstance(name, str):
        return False
    
    name = name.strip()
    company_suffixes = [
        'æœ‰é™å…¬å¸', 'æœ‰é™è´£ä»»å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸', 'é›†å›¢æœ‰é™å…¬å¸',
        'åŒ»è¯å…¬å¸', 'è¯ä¸šå…¬å¸', 'è¯æˆ¿', 'è¯Šæ‰€', 'åŒ»é™¢'
    ]
    
    return any(name.endswith(suffix) for suffix in company_suffixes)

def process_flow_data_with_fixed_matching(direct_sale_df, sales_detail_df):
    """
    ä½¿ç”¨ä¿®æ­£åçš„åŒ¹é…é€»è¾‘å¤„ç†æµå‘æ•°æ®ï¼Œå¹¶æ·»åŠ å»é‡æ£€æŸ¥å’Œå•ä½æ¢ç®—
    """
    # åˆ›å»ºåå‘æ˜ å°„
    reverse_customer_mapping, reverse_product_mapping = create_reverse_mappings()
    
    # æµå‘æ¨¡ç‰ˆåˆ—
    flow_template_cols = [
        'æµå‘å•†ä¸šå…¬å¸å', 'ä¾›è´§æ–¹', 'æ‰€å±æœˆä»½', 'å•æ®æ—¥æœŸ', 'ä»£ç†å•†', 
        'ä¸€çº§å•†ä¸šåç§°', 'äºŒçº§å•†ä¸šåç§°', 'ä¸‰çº§å•†ä¸šåç§°', 'å››çº§å•†ä¸šåç§°', 
        'ç»ˆç«¯åç§°', 'å“è§„', 'æ‰¹å·', 'é”€å”®æ•°é‡', 'è½¬æ¢åæ•°é‡', 'æ¢ç®—ç³»æ•°', 'åŸå§‹è§„æ ¼', 'æµå‘çº§åˆ«'
    ]
    
    flow_template_df = pd.DataFrame(columns=flow_template_cols)
    
    # ç”¨äºå­˜å‚¨éœ€è¦å¤„ç†çš„ä¸‹ä¸€çº§æ•°æ®
    next_level_data = []
    
    # åˆ›å»ºå·²å­˜åœ¨è®°å½•çš„é›†åˆï¼Œç”¨äºå»é‡æ£€æŸ¥
    existing_records_keys = set()
    
    # å¤„ç†4ä¸ªçº§åˆ«çš„æ•°æ®
    for level in range(1, 5):
        current_level_df = direct_sale_df[direct_sale_df['çº§æ¬¡'] == level].copy()
        
        if current_level_df.empty:
            print(f"ç¬¬ {level} çº§æ•°æ®ä¸ºç©º")
            continue
            
        print(f"å¤„ç†ç¬¬ {level} çº§æ•°æ®ï¼Œå…± {len(current_level_df)} è¡Œ")
        
        level_processed_count = 0
        
        for _, row in current_level_df.iterrows():
            try:
                # ä½¿ç”¨æ–°çš„åŒ¹é…é€»è¾‘æ‰¾åˆ°åŒ¹é…çš„é”€å”®æ•°æ®
                matched_sales = find_matching_sales_data(
                    row, sales_detail_df, reverse_customer_mapping, reverse_product_mapping
                )
                
                if matched_sales.empty:
                    continue
                
                # å¤„ç†æ¯ä¸ªåŒ¹é…çš„é”€å”®è®°å½•
                for _, sales_row in matched_sales.iterrows():
                    try:
                        # å®‰å…¨è½¬æ¢æ—¥æœŸ
                        out_date = pd.to_datetime(row['å‡ºåº“æ—¥æœŸ']) if pd.notna(row['å‡ºåº“æ—¥æœŸ']) else pd.Timestamp.now()
                        
                        # å®‰å…¨è·å–é”€å”®æ•°é‡
                        sales_quantity = pd.to_numeric(sales_row['é”€å”®æ•°é‡'], errors='coerce')
                        if pd.isna(sales_quantity):
                            sales_quantity = 0
                        
                        # è·å–è§„æ ¼ä¿¡æ¯
                        sales_spec = str(sales_row['è§„æ ¼']).strip() if 'è§„æ ¼' in sales_row and pd.notna(sales_row['è§„æ ¼']) else ''
                        out_product = str(row['äº§å“åç§°']).strip()
                        
                        # è®¡ç®—è½¬æ¢åçš„æ•°é‡
                        converted_quantity = calculate_converted_quantity(sales_quantity, out_product, sales_spec)
                        
                        # è·å–æ¢ç®—ç³»æ•°ï¼ˆç”¨äºè®°å½•ï¼‰
                        conversion_factor = get_conversion_factor(out_product, sales_spec)
                        
                        # åˆ›å»ºæµå‘è®°å½•
                        new_row = {
                            'æµå‘å•†ä¸šå…¬å¸å': str(row['å•†ä¸šå…¬å¸']).strip(),
                            'ä¾›è´§æ–¹': str(row['å•†ä¸šå…¬å¸']).strip(),
                            'æ‰€å±æœˆä»½': out_date.strftime('%Y-%m'),
                            'å•æ®æ—¥æœŸ': out_date,
                            'ä»£ç†å•†': str(row['å•†ä¸šå…¬å¸']).strip(),
                            'ä¸€çº§å•†ä¸šåç§°': '',
                            'äºŒçº§å•†ä¸šåç§°': '',
                            'ä¸‰çº§å•†ä¸šåç§°': '',
                            'å››çº§å•†ä¸šåç§°': '',
                            'ç»ˆç«¯åç§°': str(sales_row['å®¢æˆ·åç§°']).strip(),
                            'å“è§„': str(row['äº§å“åç§°']).strip(),
                            'æ‰¹å·': str(row['æ‰¹å·']).strip(),
                            'é”€å”®æ•°é‡': sales_quantity,
                            'è½¬æ¢åæ•°é‡': converted_quantity,
                            'æ¢ç®—ç³»æ•°': conversion_factor,
                            'åŸå§‹è§„æ ¼': sales_spec,
                            'æµå‘çº§åˆ«': level
                        }
                        
                        # è®¾ç½®å¯¹åº”çº§åˆ«çš„å•†ä¸šåç§°
                        level_key = f'{["", "ä¸€", "äºŒ", "ä¸‰", "å››"][level]}çº§å•†ä¸šåç§°'
                        new_row[level_key] = str(row['å•†ä¸šå…¬å¸']).strip()
                        
                        # æ·»åŠ åˆ°ç»“æœDataFrame
                        flow_template_df = pd.concat([flow_template_df, pd.DataFrame([new_row])], ignore_index=True)
                        level_processed_count += 1
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºä¸‹ä¸€çº§æ•°æ® - ä½¿ç”¨è½¬æ¢åçš„æ•°é‡
                        customer_name = str(sales_row['å®¢æˆ·åç§°']).strip()
                        if level < 4 and is_company_like(customer_name):
                            # åˆ›å»ºä¸‹ä¸€çº§å‡ºåº“è®°å½•
                            next_level_row = {
                                'å‡ºåº“æ—¥æœŸ': row['å‡ºåº“æ—¥æœŸ'],
                                'å•†ä¸šå…¬å¸': customer_name,
                                'äº§å“åç§°': row['äº§å“åç§°'],
                                'æ‰¹å·': row['æ‰¹å·'],
                                'æ•°é‡': converted_quantity,  # ä½¿ç”¨è½¬æ¢åçš„æ•°é‡
                                'çº§æ¬¡': level + 1
                            }
                            
                            # æ£€æŸ¥æ˜¯å¦é‡å¤
                            new_record_key = create_record_key(next_level_row)
                            
                            if new_record_key is not None:
                                # æ£€æŸ¥åœ¨ç°æœ‰direct_sale_dfä¸­æ˜¯å¦å·²å­˜åœ¨
                                if not is_duplicate_record(next_level_row, direct_sale_df):
                                    # æ£€æŸ¥åœ¨å¾…æ·»åŠ çš„next_level_dataä¸­æ˜¯å¦å·²å­˜åœ¨
                                    if new_record_key not in existing_records_keys:
                                        next_level_data.append(next_level_row)
                                        existing_records_keys.add(new_record_key)
                                        print(f"æ·»åŠ ä¸‹ä¸€çº§è®°å½•: çº§æ¬¡{level + 1}, å…¬å¸:{customer_name}, äº§å“:{row['äº§å“åç§°']}, æ‰¹å·:{row['æ‰¹å·']}, æ•°é‡:{converted_quantity}")
                                    else:
                                        print(f"è·³è¿‡é‡å¤è®°å½•(å¾…æ·»åŠ é˜Ÿåˆ—): çº§æ¬¡{level + 1}, å…¬å¸:{customer_name}, äº§å“:{row['äº§å“åç§°']}, æ‰¹å·:{row['æ‰¹å·']}")
                                else:
                                    print(f"è·³è¿‡é‡å¤è®°å½•(å·²å­˜åœ¨): çº§æ¬¡{level + 1}, å…¬å¸:{customer_name}, äº§å“:{row['äº§å“åç§°']}, æ‰¹å·:{row['æ‰¹å·']}")
                    
                    except Exception as e:
                        print(f"å¤„ç†é”€å”®è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
                        continue
            
            except Exception as e:
                print(f"å¤„ç†å‡ºåº“è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"ç¬¬ {level} çº§å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {level_processed_count} æ¡æµå‘è®°å½•")
        
        # å°†ä¸‹ä¸€çº§æ•°æ®æ·»åŠ åˆ°direct_sale_dfä¸­
        if next_level_data:
            next_level_df = pd.DataFrame(next_level_data)
            direct_sale_df = pd.concat([direct_sale_df, next_level_df], ignore_index=True)
            
            # æ›´æ–°å·²å­˜åœ¨è®°å½•çš„é”®é›†åˆ
            for record in next_level_data:
                record_key = create_record_key(record)
                if record_key:
                    existing_records_keys.add(record_key)
                    
            print(f"æ·»åŠ äº† {len(next_level_data)} æ¡ä¸‹ä¸€çº§è®°å½•åˆ°å¤„ç†é˜Ÿåˆ—")
            next_level_data = []  # æ¸…ç©ºåˆ—è¡¨
    
    print(f"æµå‘æ•°æ®å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(flow_template_df)} æ¡è®°å½•")
    return flow_template_df

def read_excel_file(file_path):
    """è¯»å–Excelæ–‡ä»¶ï¼Œæ”¯æŒ.xlsxå’Œ.xlsæ ¼å¼"""
    try:
        if file_path.endswith('.xlsx'):
            return pd.read_excel(file_path, engine='openpyxl')
        elif file_path.endswith('.xls'):
            # å°è¯•ä½¿ç”¨openpyxlï¼Œå¦‚æœä¸æ”¯æŒåˆ™ä½¿ç”¨xlrd
            try:
                return pd.read_excel(file_path, engine='openpyxl')
            except Exception:
                try:
                    return pd.read_excel(file_path, engine='xlrd')
                except Exception:
                    # å¦‚æœxlrdä¹Ÿä¸å¯ç”¨ï¼Œå°è¯•é»˜è®¤å¼•æ“
                    return pd.read_excel(file_path)
        else:
            return pd.read_excel(file_path)
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None

def safe_delete_file(file_path):
    """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶å ç”¨é—®é¢˜"""
    try:
        if file_path and os.path.exists(file_path):
            os.unlink(file_path)
            return True
    except Exception as e:
        print(f"æ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}: {e}")
        return False

def safe_delete_directory(dir_path):
    """å®‰å…¨åˆ é™¤ç›®å½•"""
    try:
        if dir_path and os.path.exists(dir_path):
            shutil.rmtree(dir_path, ignore_errors=True)
            return True
    except Exception as e:
        print(f"æ— æ³•åˆ é™¤ç›®å½• {dir_path}: {e}")
        return False

def safe_convert_date(date_value):
    """å®‰å…¨è½¬æ¢æ—¥æœŸ"""
    try:
        if pd.isna(date_value):
            return None
        if isinstance(date_value, str):
            return pd.to_datetime(date_value)
        elif isinstance(date_value, datetime):
            return date_value
        else:
            return pd.to_datetime(date_value)
    except Exception:
        return None

def find_column_mapping(df_columns):
    """æ™ºèƒ½åŒ¹é…åˆ—å"""
    mapping = {}
    
    # å®šä¹‰å¯èƒ½çš„åˆ—åæ˜ å°„
    column_mappings = {
        'date': ['å‡ºåº“æ—¥æœŸ', 'æ“ä½œæ—¥æœŸ', 'æ—¥æœŸ', 'å•æ®æ—¥æœŸ'],
        'company': ['å•†ä¸šå…¬å¸', 'è´­è´§å•ä½', 'å®¢æˆ·', 'å…¬å¸', 'å…¬å¸åç§°'],
        'product': ['äº§å“åç§°', 'äº§å“', 'å•†å“åç§°', 'å“å'],
        'batch': ['æ‰¹å·'],
        'quantity': ['æ•°é‡', 'æ‰¹å·å‡ºåº“æ•°é‡', 'å‡ºåº“æ•°é‡', 'é”€å”®æ•°é‡']
    }
    
    # è½¬æ¢åˆ—åä¸ºå­—ç¬¦ä¸²ï¼Œä¾¿äºæ¯”è¾ƒ
    df_columns_str = [str(col).strip() for col in df_columns]
    
    for col_str in df_columns_str:
        for key, possible_names in column_mappings.items():
            if col_str in possible_names and key not in mapping:
                mapping[key] = col_str
                break
    
    return mapping

def validate_required_columns(sales_df, required_cols=['å…¬å¸åç§°', 'å•†å“åç§°', 'æ‰¹å·', 'é”€å”®æ•°é‡', 'å®¢æˆ·åç§°']):
    """éªŒè¯é”€å”®æ˜ç»†è¡¨æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—"""
    missing_cols = []
    for col in required_cols:
        if col not in sales_df.columns:
            missing_cols.append(col)
    
    return missing_cols

def process_files(zip_file, sales_file):
    """å¤„ç†æ–‡ä»¶çš„ä¸»è¦é€»è¾‘"""
    # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    temp_files = {}
    
    try:
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("è¯»å–é”€å”®æ˜ç»†è¡¨...")
        progress_bar.progress(10)
        
        # è¯»å–é”€å”®æ˜ç»†è¡¨
        try:
            # ä¿å­˜é”€å”®æ–‡ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
            sales_temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')
            temp_files['sales_file'] = sales_temp_file.name
            sales_temp_file.write(sales_file.getvalue())
            sales_temp_file.close()
            
            sales_detail_df = read_excel_file(temp_files['sales_file'])
            if sales_detail_df is None:
                st.error("æ— æ³•è¯»å–é”€å”®æ˜ç»†è¡¨")
                return None, None
                
            # æ¸…ç†åˆ—å
            sales_detail_df.columns = sales_detail_df.columns.str.strip()
            
            # éªŒè¯å¿…éœ€åˆ—
            missing_cols = validate_required_columns(sales_detail_df)
            if missing_cols:
                st.error(f"é”€å”®æ˜ç»†è¡¨ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
                return None, None
            
            st.success(f"é”€å”®æ˜ç»†è¡¨è¯»å–æˆåŠŸï¼Œå…± {len(sales_detail_df)} è¡Œæ•°æ®")
            
        except Exception as e:
            st.error(f"è¯»å–é”€å”®æ˜ç»†è¡¨å¤±è´¥: {e}")
            return None, None
        
        status_text.text("è§£å‹å¹¶å¤„ç†å‡ºåº“æ˜ç»†å‹ç¼©åŒ…...")
        progress_bar.progress(20)
        
        # ä¿å­˜ä¸Šä¼ çš„å‹ç¼©åŒ…åˆ°ä¸´æ—¶æ–‡ä»¶
        zip_temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
        temp_files['zip_file'] = zip_temp_file.name
        zip_temp_file.write(zip_file.getvalue())
        zip_temp_file.close()
        
        # è§£å‹å¹¶å¤„ç†å‡ºåº“æ˜ç»†å‹ç¼©åŒ…
        try:
            extract_dir = tempfile.mkdtemp()
            temp_files['extract_dir'] = extract_dir
            
            with zipfile.ZipFile(temp_files['zip_file'], 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
                
            excel_files = [f for f in os.listdir(extract_dir) if f.endswith(('.xlsx', '.xls'))]
            st.info(f"å‹ç¼©åŒ…ä¸­åŒ…å« {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
            
            if not excel_files:
                st.warning("å‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°Excelæ–‡ä»¶")
                return None, None
            
            # åˆå§‹åŒ–ç›´é”€å•†ä¸šæ•°æ®
            direct_sale_cols = ['å‡ºåº“æ—¥æœŸ', 'å•†ä¸šå…¬å¸', 'äº§å“åç§°', 'æ‰¹å·', 'æ•°é‡', 'çº§æ¬¡']
            direct_sale_df = pd.DataFrame(columns=direct_sale_cols)
            
            total_files = len(excel_files)
            processed_files = 0
            
            # éå†å‹ç¼©åŒ…ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for file_name in excel_files:
                file_path = os.path.join(extract_dir, file_name)
                st.info(f"å¤„ç†æ–‡ä»¶: {file_name}")
                
                try:
                    # è¯»å–Excelæ–‡ä»¶
                    df = read_excel_file(file_path)
                    if df is None or df.empty:
                        st.warning(f"æ–‡ä»¶ {file_name} ä¸ºç©ºæˆ–æ— æ³•è¯»å–")
                        continue
                    
                    # æ™ºèƒ½åŒ¹é…åˆ—å
                    column_mapping = find_column_mapping(df.columns)
                    
                    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                    required_cols = ['date', 'company', 'product', 'batch', 'quantity']
                    missing_cols = [col for col in required_cols if col not in column_mapping]
                    
                    if missing_cols:
                        st.warning(f"æ–‡ä»¶ {file_name} ç¼ºå°‘å¿…è¦çš„åˆ—æ˜ å°„: {missing_cols}")
                        continue
                    
                    # æå–éœ€è¦çš„åˆ—
                    selected_df = df[[
                        column_mapping['date'],
                        column_mapping['company'],
                        column_mapping['product'],
                        column_mapping['batch'],
                        column_mapping['quantity']
                    ]].copy()
                    
                    # é‡å‘½ååˆ—
                    selected_df.columns = ['å‡ºåº“æ—¥æœŸ', 'å•†ä¸šå…¬å¸', 'äº§å“åç§°', 'æ‰¹å·', 'æ•°é‡']
                    
                    # æ•°æ®æ¸…æ´—
                    selected_df = selected_df.dropna(subset=['å•†ä¸šå…¬å¸', 'äº§å“åç§°', 'æ‰¹å·'])
                    selected_df['æ•°é‡'] = pd.to_numeric(selected_df['æ•°é‡'], errors='coerce').fillna(0)
                    
                    # æ·»åŠ çº§æ¬¡åˆ—
                    selected_df['çº§æ¬¡'] = 1
                    
                    # åˆå¹¶åˆ°ç›´é”€å•†ä¸šè¡¨
                    direct_sale_df = pd.concat([direct_sale_df, selected_df], ignore_index=True)
                    
                    processed_files += 1
                    progress_bar.progress(20 + int(60 * processed_files / total_files))
                    
                except Exception as e:
                    st.warning(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
                    continue
            
            st.success(f"æˆåŠŸå¤„ç† {processed_files}/{total_files} ä¸ªæ–‡ä»¶ï¼Œè·å¾— {len(direct_sale_df)} æ¡å‡ºåº“è®°å½•")
            
        except Exception as e:
            st.error(f"è§£å‹æˆ–å¤„ç†å‹ç¼©åŒ…å¤±è´¥: {e}")
            return None, None
        
        if direct_sale_df.empty:
            st.warning("æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆçš„å‡ºåº“æ•°æ®")
            return None, None
        
        status_text.text("å¤„ç†æµå‘æ•°æ®...")
        progress_bar.progress(80)
        
        # å¤„ç†æµå‘æ•°æ®
        try:
            # ä½¿ç”¨ä¿®æ­£åçš„åŒ¹é…é€»è¾‘å’Œå•ä½æ¢ç®—
            flow_template_df = process_flow_data_with_fixed_matching(direct_sale_df, sales_detail_df)
            
            if flow_template_df.empty:
                st.warning("æœªç”Ÿæˆä»»ä½•æµå‘æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®åŒ¹é…æƒ…å†µ")
                st.info("å¯èƒ½çš„åŸå› ï¼š")
                st.info("1. å‡ºåº“æ˜ç»†å’Œé”€å”®æ˜ç»†ä¸­çš„å…¬å¸åç§°ä¸åŒ¹é…")
                st.info("2. äº§å“åç§°æ˜ å°„é…ç½®ä¸æ­£ç¡®")  
                st.info("3. æ‰¹å·ä¸åŒ¹é…")
                return None, None
            
            # ä¿å­˜ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
            result_temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')
            temp_files['result_file'] = result_temp_file.name
            result_temp_file.close()
            
            flow_template_df.to_excel(temp_files['result_file'], index=False)
            
            progress_bar.progress(100)
            status_text.text("å¤„ç†å®Œæˆï¼")
            
            # è¿”å›ç»“æœæ–‡ä»¶è·¯å¾„å’Œä¸´æ—¶æ–‡ä»¶å­—å…¸
            return temp_files['result_file'], temp_files
            
        except Exception as e:
            st.error(f"å¤„ç†æµå‘æ•°æ®å¤±è´¥: {e}")
            return None, None
            
    except Exception as e:
        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None, None

def cleanup_temp_files(temp_files):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    if not temp_files:
        return
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for key, file_path in temp_files.items():
        if key == 'extract_dir':
            safe_delete_directory(file_path)
        elif key != 'result_file':  # result_file ç”±è°ƒç”¨è€…å¤„ç†
            safe_delete_file(file_path)

def main():
    """ä¸»å‡½æ•°"""
    st.title("ğŸ“Š æµå‘æ•°æ®å¤„ç†AIç³»ç»Ÿ")
    st.markdown("---")
    
    # æ˜¾ç¤ºäº§å“æ˜ å°„ä¿¡æ¯
    with st.expander("ğŸ“‹ æŸ¥çœ‹äº§å“æ˜ å°„é…ç½®"):
        st.write("å½“å‰æ”¯æŒçš„äº§å“æ˜ å°„ï¼š")
        for out_name, mapping in product_mapping.items():
            st.write(f"**{out_name}**")
            st.write(f"- å•†å“åç§°: {', '.join(mapping['å•†å“åç§°'])}")
            st.write(f"- è§„æ ¼: {', '.join(mapping['è§„æ ¼'])}")
            st.write("- å•ä½æ¢ç®—ç³»æ•°:")
            for spec, factor in mapping['å•ä½æ¢ç®—ç³»æ•°'].items():
                st.write(f"  - {spec}: {factor}")
            st.write("")
    
    # æ˜¾ç¤ºå®¢æˆ·æ˜ å°„ä¿¡æ¯
    with st.expander("ğŸ¢ æŸ¥çœ‹å®¢æˆ·æ˜ å°„é…ç½®"):
        st.write("å½“å‰æ”¯æŒçš„å®¢æˆ·æ˜ å°„ï¼š")
        for short_name, full_name in customer_alias_mapping.items():
            st.write(f"**{short_name}** â†’ {full_name}")
    
    st.markdown("---")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¦  å‚å®¶å‡ºåº“æ˜ç»†å‹ç¼©åŒ…")
        st.info("ä¸Šä¼ åŒ…å«å‡ºåº“æ˜ç»†Excelæ–‡ä»¶çš„ZIPå‹ç¼©åŒ…")
        zip_file = st.file_uploader("ä¸Šä¼ å‡ºåº“æ˜ç»†å‹ç¼©åŒ… (.zip)", type=['zip'], key='zip_uploader')
    
    with col2:
        st.subheader("ğŸ“‹ å•†ä¸šå…¬å¸é”€å”®æ˜ç»†è¡¨")
        st.info("ä¸Šä¼ å•†ä¸šå…¬å¸é”€å”®æ˜ç»†Excelæ–‡ä»¶")
        sales_file = st.file_uploader("ä¸Šä¼ å•†ä¸šå…¬å¸é”€å”®æ˜ç»†è¡¨ (.xlsx)", type=['xlsx'], key='sales_uploader')
    
    # å¤„ç†æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
        if zip_file is None or sales_file is None:
            st.warning("è¯·å…ˆä¸Šä¼ æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶ï¼")
            return
        
        # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
        temp_files_to_cleanup = None
        result_file_path = None
        
        try:
            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™..."):
                result_file_path, temp_files_to_cleanup = process_files(zip_file, sales_file)
                
                if result_file_path:
                    # è¯»å–ç»“æœæ–‡ä»¶
                    try:
                        result_df = pd.read_excel(result_file_path)
                        
                        # æ˜¾ç¤ºå¤„ç†ç»“æœ
                        st.success("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        col1, col2, col3, col4, col5 = st.columns(5)
                        
                        with col1:
                            st.metric("æ€»è®°å½•æ•°", len(result_df))
                        
                        with col2:
                            unique_companies = result_df['æµå‘å•†ä¸šå…¬å¸å'].nunique()
                            st.metric("æ¶‰åŠå•†ä¸šå…¬å¸æ•°", unique_companies)
                        
                        with col3:
                            unique_products = result_df['å“è§„'].nunique()
                            st.metric("æ¶‰åŠäº§å“æ•°", unique_products)
                            
                        with col4:
                            total_original_quantity = result_df['é”€å”®æ•°é‡'].sum()
                            st.metric("åŸå§‹é”€å”®æ•°é‡", f"{total_original_quantity:,.0f}")
                            
                        with col5:
                            total_converted_quantity = result_df['è½¬æ¢åæ•°é‡'].sum()
                            st.metric("è½¬æ¢åé”€å”®æ•°é‡", f"{total_converted_quantity:,.0f}")
                        
                        # æŒ‰æµå‘çº§åˆ«ç»Ÿè®¡
                        st.subheader("ğŸ“ˆ æµå‘çº§åˆ«ç»Ÿè®¡")
                        level_stats = result_df.groupby('æµå‘çº§åˆ«').agg({
                            'æµå‘å•†ä¸šå…¬å¸å': 'count',
                            'é”€å”®æ•°é‡': 'sum',
                            'è½¬æ¢åæ•°é‡': 'sum'
                        }).rename(columns={
                            'æµå‘å•†ä¸šå…¬å¸å': 'è®°å½•æ•°',
                            'é”€å”®æ•°é‡': 'åŸå§‹æ€»æ•°é‡',
                            'è½¬æ¢åæ•°é‡': 'è½¬æ¢åæ€»æ•°é‡'
                        })
                        st.dataframe(level_stats)
                        
                        # æŒ‰äº§å“ç»Ÿè®¡æ¢ç®—ç³»æ•°ä½¿ç”¨æƒ…å†µ
                        st.subheader("ğŸ”„ å•ä½æ¢ç®—ç»Ÿè®¡")
                        conversion_stats = result_df.groupby(['å“è§„', 'æ¢ç®—ç³»æ•°', 'åŸå§‹è§„æ ¼']).agg({
                            'é”€å”®æ•°é‡': ['count', 'sum'],
                            'è½¬æ¢åæ•°é‡': 'sum'
                        }).round(2)
                        conversion_stats.columns = ['è®°å½•æ•°', 'åŸå§‹æ•°é‡åˆè®¡', 'è½¬æ¢åæ•°é‡åˆè®¡']
                        st.dataframe(conversion_stats)
                        
                        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                        st.subheader("ğŸ“Š å¤„ç†ç»“æœé¢„è§ˆ")
                        st.dataframe(result_df.head(10))
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        st.subheader("ğŸ“¥ ä¸‹è½½å¤„ç†ç»“æœ")
                        
                        # åˆ›å»ºä¸‹è½½æŒ‰é’®
                        output = BytesIO()
                        with pd.ExcelWriter(output, engine='openpyxl') as writer:
                            result_df.to_excel(writer, index=False, sheet_name='æµå‘æ•°æ®')
                            
                            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯å·¥ä½œè¡¨
                            level_stats.to_excel(writer, sheet_name='çº§åˆ«ç»Ÿè®¡')
                            conversion_stats.to_excel(writer, sheet_name='æ¢ç®—ç»Ÿè®¡')
                            
                            # æ·»åŠ é…ç½®ä¿¡æ¯å·¥ä½œè¡¨
                            config_data = []
                            config_data.append(['ç±»å‹', 'ç®€ç§°/å•†å“å', 'å…¨ç§°/è§„æ ¼', 'æ¢ç®—ç³»æ•°'])
                            config_data.append(['', '', '', ''])
                            config_data.append(['å®¢æˆ·æ˜ å°„', '', '', ''])
                            for short, full in customer_alias_mapping.items():
                                config_data.append(['å®¢æˆ·', short, full, ''])
                            
                            config_data.append(['', '', '', ''])
                            config_data.append(['äº§å“æ˜ å°„', '', '', ''])
                            for out_name, mapping in product_mapping.items():
                                config_data.append(['äº§å“', out_name, '', ''])
                                for sales_name in mapping['å•†å“åç§°']:
                                    config_data.append(['', f'  å•†å“å: {sales_name}', '', ''])
                                for spec in mapping['è§„æ ¼']:
                                    factor = mapping['å•ä½æ¢ç®—ç³»æ•°'].get(spec, mapping['å•ä½æ¢ç®—ç³»æ•°']['default'])
                                    config_data.append(['', f'  è§„æ ¼: {spec}', '', str(factor)])
                            
                            config_df = pd.DataFrame(config_data[1:], columns=config_data[0])
                            config_df.to_excel(writer, index=False, sheet_name='é…ç½®ä¿¡æ¯')
                        
                        output.seek(0)
                        
                        # ä¸‹è½½æŒ‰é’®
                        download_filename = f"æµå‘æ•°æ®_å¸¦å•ä½æ¢ç®—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                        
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½æ¸…æ´—å®Œæˆæµå‘æ•°æ®Excelæ–‡ä»¶",
                            data=output.getvalue(),
                            file_name=download_filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                        
                        # æˆåŠŸæç¤º
                        st.success(f"æ–‡ä»¶å·²å‡†å¤‡å¥½ä¸‹è½½ï¼š{download_filename}")
                        
                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        st.subheader("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
                        null_counts = result_df.isnull().sum()
                        if null_counts.sum() > 0:
                            st.warning("å‘ç°ç©ºå€¼æ•°æ®ï¼š")
                            for col, count in null_counts[null_counts > 0].items():
                                st.write(f"- {col}: {count} ä¸ªç©ºå€¼")
                        else:
                            st.success("âœ… æ•°æ®å®Œæ•´ï¼Œæ— ç©ºå€¼")
                        
                        # æ£€æŸ¥æ•°é‡å¼‚å¸¸
                        zero_quantity = len(result_df[result_df['é”€å”®æ•°é‡'] <= 0])
                        if zero_quantity > 0:
                            st.warning(f"å‘ç° {zero_quantity} æ¡é”€å”®æ•°é‡ä¸º0æˆ–è´Ÿæ•°çš„è®°å½•")
                        else:
                            st.success("âœ… é”€å”®æ•°é‡æ•°æ®æ­£å¸¸")
                        
                        # æ£€æŸ¥æ¢ç®—ç³»æ•°å¼‚å¸¸
                        abnormal_factors = result_df[result_df['æ¢ç®—ç³»æ•°'] <= 0]
                        if not abnormal_factors.empty:
                            st.warning(f"å‘ç° {len(abnormal_factors)} æ¡æ¢ç®—ç³»æ•°å¼‚å¸¸çš„è®°å½•")
                        else:
                            st.success("âœ… æ¢ç®—ç³»æ•°æ•°æ®æ­£å¸¸")
                        
                        # æ˜¾ç¤ºå»é‡ç»Ÿè®¡ä¿¡æ¯
                        st.subheader("ğŸ“„ å»é‡ç»Ÿè®¡ä¿¡æ¯")
                        st.info("ç³»ç»Ÿå·²è‡ªåŠ¨æ£€æŸ¥å¹¶é¿å…äº†é‡å¤çš„å‡ºåº“è®°å½•ç”Ÿæˆ")
                        st.write("å»é‡æ£€æŸ¥æ¡ä»¶ï¼šå‡ºåº“æ—¥æœŸã€å•†ä¸šå…¬å¸ã€äº§å“åç§°ã€æ‰¹å·ã€æ•°é‡å®Œå…¨ç›¸åŒçš„è®°å½•")
                        
                        # æ˜¾ç¤ºå•ä½æ¢ç®—è¯´æ˜
                        st.subheader("ğŸ”„ å•ä½æ¢ç®—è¯´æ˜")
                        st.info("ç³»ç»Ÿæ ¹æ®äº§å“è§„æ ¼è‡ªåŠ¨è¿›è¡Œäº†å•ä½æ¢ç®—ï¼Œç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§")
                        st.write("æ¢ç®—é€»è¾‘ï¼š")
                        st.write("- é”€å”®æ•°é‡ï¼šé”€å”®æ˜ç»†ä¸­çš„åŸå§‹æ•°é‡")
                        st.write("- æ¢ç®—ç³»æ•°ï¼šæ ¹æ®äº§å“è§„æ ¼ç¡®å®šçš„è½¬æ¢å€æ•°")
                        st.write("- è½¬æ¢åæ•°é‡ï¼šé”€å”®æ•°é‡ Ã— æ¢ç®—ç³»æ•°")
                        st.write("- è½¬æ¢åçš„æ•°é‡ç”¨äºç”Ÿæˆä¸‹ä¸€çº§æµå‘æ•°æ®")
                            
                    except Exception as e:
                        st.error(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
                else:
                    st.error("âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œæ•°æ®å†…å®¹")
                    
        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆé™¤äº†ç»“æœæ–‡ä»¶ï¼‰
            if temp_files_to_cleanup:
                cleanup_temp_files(temp_files_to_cleanup)
            
            # æ¸…ç†ç»“æœæ–‡ä»¶
            if result_file_path:
                safe_delete_file(result_file_path)

    # ä½¿ç”¨è¯´æ˜
    st.markdown("---")
    st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")
    
    with st.expander("ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è¯´æ˜"):
        st.markdown("""
        ### æ–‡ä»¶è¦æ±‚
        
        **å‡ºåº“æ˜ç»†å‹ç¼©åŒ…ï¼š**
        - æ ¼å¼ï¼šZIPå‹ç¼©åŒ…
        - å†…å®¹ï¼šåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªExcelæ–‡ä»¶ï¼ˆ.xlsx æˆ– .xlsï¼‰
        - å¿…éœ€åˆ—ï¼šå‡ºåº“æ—¥æœŸã€å•†ä¸šå…¬å¸ã€äº§å“åç§°ã€æ‰¹å·ã€æ•°é‡ï¼ˆåˆ—åå¯ä»¥æœ‰å˜åŒ–ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ï¼‰
        
        **é”€å”®æ˜ç»†è¡¨ï¼š**
        - æ ¼å¼ï¼šExcelæ–‡ä»¶ï¼ˆ.xlsxï¼‰
        - å¿…éœ€åˆ—ï¼šå…¬å¸åç§°ã€å•†å“åç§°ã€æ‰¹å·ã€é”€å”®æ•°é‡ã€å®¢æˆ·åç§°ã€è§„æ ¼ï¼ˆå¯é€‰ï¼‰
        
        ### æ•°æ®å¤„ç†é€»è¾‘
        
        1. **å…¬å¸åç§°åŒ¹é…**ï¼šæ ¹æ®é…ç½®çš„æ˜ å°„å…³ç³»ï¼Œå°†é”€å”®æ˜ç»†ä¸­çš„å…¬å¸ç®€ç§°åŒ¹é…åˆ°å‡ºåº“æ˜ç»†ä¸­çš„å…¨ç§°
        2. **äº§å“åç§°åŒ¹é…**ï¼šæ ¹æ®é…ç½®çš„äº§å“æ˜ å°„ï¼ŒåŒ¹é…ä¸åŒè¡¨ä¸­çš„äº§å“åç§°å’Œè§„æ ¼
        3. **æ‰¹å·ç²¾ç¡®åŒ¹é…**ï¼šç¡®ä¿æµå‘æ•°æ®çš„å‡†ç¡®æ€§
        4. **å•ä½æ¢ç®—å¤„ç†**ï¼šæ ¹æ®äº§å“è§„æ ¼è‡ªåŠ¨è¿›è¡Œå•ä½æ¢ç®—ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        5. **å¤šçº§æµå‘ç”Ÿæˆ**ï¼šè‡ªåŠ¨è¯†åˆ«ä¸‹æ¸¸å…¬å¸ï¼Œç”Ÿæˆå¤šçº§æµå‘å…³ç³»
        6. **å»é‡æ£€æŸ¥**ï¼šé¿å…ç”Ÿæˆé‡å¤çš„å‡ºåº“è®°å½•ï¼Œæ£€æŸ¥æ¡ä»¶åŒ…æ‹¬å‡ºåº“æ—¥æœŸã€å•†ä¸šå…¬å¸ã€äº§å“åç§°ã€æ‰¹å·ã€æ•°é‡
        
        ### å•ä½æ¢ç®—åŠŸèƒ½
        
        **æ–°å¢åŠŸèƒ½è¯´æ˜ï¼š**
        - **æ¢ç®—ç³»æ•°é…ç½®**ï¼šæ¯ä¸ªäº§å“å¯é…ç½®ä¸åŒè§„æ ¼çš„æ¢ç®—ç³»æ•°
        - **è‡ªåŠ¨å•ä½æ¢ç®—**ï¼šç³»ç»Ÿè‡ªåŠ¨æ ¹æ®è§„æ ¼è¿›è¡Œæ•°é‡è½¬æ¢
        - **æ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿ä¸Šä¸‹æ¸¸æµå‘æ•°æ®çš„å•ä½ç»Ÿä¸€
        - **æ¢ç®—è®°å½•è¿½æº¯**ï¼šä¿ç•™åŸå§‹æ•°é‡ã€æ¢ç®—ç³»æ•°å’Œè½¬æ¢åæ•°é‡ï¼Œä¾¿äºå®¡è®¡
        
        **æ¢ç®—é€»è¾‘ï¼š**
        - åŸå§‹é”€å”®æ•°é‡æ¥è‡ªé”€å”®æ˜ç»†
        - æ ¹æ®äº§å“å’Œè§„æ ¼æŸ¥æ‰¾å¯¹åº”çš„æ¢ç®—ç³»æ•°
        - è½¬æ¢åæ•°é‡ = åŸå§‹é”€å”®æ•°é‡ Ã— æ¢ç®—ç³»æ•°
        - è½¬æ¢åçš„æ•°é‡ç”¨äºç”Ÿæˆä¸‹ä¸€çº§æµå‘æ•°æ®
        
        ### è¾“å‡ºç»“æœ
        
        - **æµå‘æ•°æ®**ï¼šåŒ…å«å®Œæ•´çš„æµå‘ä¿¡æ¯ï¼Œæ”¯æŒæœ€å¤š4çº§æµå‘ï¼ŒåŒ…å«å•ä½æ¢ç®—ä¿¡æ¯
        - **çº§åˆ«ç»Ÿè®¡**ï¼šå„çº§åˆ«çš„è®°å½•æ•°å’Œé”€å”®æ•°é‡ç»Ÿè®¡ï¼ˆåŸå§‹å’Œè½¬æ¢åï¼‰
        - **æ¢ç®—ç»Ÿè®¡**ï¼šå„äº§å“çš„æ¢ç®—ç³»æ•°ä½¿ç”¨æƒ…å†µç»Ÿè®¡
        - **é…ç½®ä¿¡æ¯**ï¼šå½“å‰ä½¿ç”¨çš„æ˜ å°„é…ç½®ï¼Œä¾¿äºæ£€æŸ¥å’Œè°ƒè¯•
        
        ### æ³¨æ„äº‹é¡¹
        
        - ç¡®ä¿æ•°æ®ä¸­çš„å…¬å¸åç§°å’Œäº§å“åç§°åœ¨æ˜ å°„é…ç½®ä¸­æœ‰å¯¹åº”å…³ç³»
        - æ‰¹å·å¿…é¡»å®Œå…¨ä¸€è‡´æ‰èƒ½åŒ¹é…æˆåŠŸ
        - ç³»ç»Ÿä¼šè‡ªåŠ¨è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼Œå¦‚ç©ºå€¼å’Œå¼‚å¸¸æ•°é‡
        - å¤„ç†å¤§é‡æ•°æ®æ—¶è¯·è€å¿ƒç­‰å¾…ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºå¤„ç†è¿›åº¦
        - **é‡è¦**ï¼šç³»ç»Ÿå·²åŠ å…¥å»é‡æœºåˆ¶ï¼Œé¿å…åŒä¸€å‡ºåº“è®°å½•è¢«é‡å¤å¤„ç†
        - **æ–°å¢**ï¼šå•ä½æ¢ç®—åŠŸèƒ½ç¡®ä¿äº†æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯è¿½æº¯æ€§
        
        ### å»é‡æœºåˆ¶è¯´æ˜
        
        ç³»ç»Ÿä¼šæ£€æŸ¥ä»¥ä¸‹å­—æ®µçš„ç»„åˆæ¥åˆ¤æ–­è®°å½•æ˜¯å¦é‡å¤ï¼š
        - å‡ºåº“æ—¥æœŸ
        - å•†ä¸šå…¬å¸
        - äº§å“åç§°  
        - æ‰¹å·
        - æ•°é‡
        
        å½“è¿™5ä¸ªå­—æ®µå®Œå…¨ç›¸åŒæ—¶ï¼Œç³»ç»Ÿä¼šè·³è¿‡è¯¥è®°å½•çš„æ·»åŠ ï¼Œé¿å…æ•°æ®é‡å¤ã€‚
        
        ### å•ä½æ¢ç®—ç³»æ•°é…ç½®è¯´æ˜
        
        æ¯ä¸ªäº§å“å¯ä»¥é…ç½®å¤šä¸ªè§„æ ¼å¯¹åº”çš„æ¢ç®—ç³»æ•°ï¼š
        - **ç²¾ç¡®åŒ¹é…**ï¼šä¼˜å…ˆä½¿ç”¨ä¸é”€å”®æ˜ç»†è§„æ ¼å®Œå…¨åŒ¹é…çš„æ¢ç®—ç³»æ•°
        - **é»˜è®¤ç³»æ•°**ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤æ¢ç®—ç³»æ•°
        - **ç³»æ•°ä¸º1**ï¼šå¦‚æœäº§å“ä¸åœ¨é…ç½®ä¸­ï¼Œé»˜è®¤ä½¿ç”¨æ¢ç®—ç³»æ•°1ï¼ˆä¸è¿›è¡Œæ¢ç®—ï¼‰
        - **æ•°æ®è¿½æº¯**ï¼šè¾“å‡ºç»“æœä¸­åŒ…å«åŸå§‹æ•°é‡ã€æ¢ç®—ç³»æ•°å’Œè½¬æ¢åæ•°é‡ï¼Œä¾¿äºæ•°æ®å®¡è®¡
        """)

if __name__ == "__main__":
    main()